---
sidebar_position: 1
---

# D31S1. Data Pipelines

## Data platforms

Modern organizations extract value from tons of data they gather. They used to build environments called "data platforms," centralised data stores where useful data was collected and made available to approved people. A data platform is an integrated set of technologies that collectively meet an organization's data needs. It enables the acquisition, storage, preparation, delivery, and governance of data. Data platforms also provide data security layers for users and applications.

## Data pipelines

Most organisations are a dynamic mesh of data connections which need to be continually maintained and updated. Hence, the concept of a data platform has been replaced with that of a data pipeline.

Data pipelines are a mix of code and infrastructure. Pipelines include at least a set of databases, tables, attributes, buckets, roles, etc.

There are two main types of data pipelines:

- [ETL](#etl)
- [ELT](#elt)

## ETL

ETL means **extract-transform-load.**

This traditional approach to data integration, known as extract-transform-load (ETL), has been around since the 1970s. In the ETL process, data is extracted in bulk from various sources, transformed into a desired format, then loaded for storage into its target destination.

1. Extraction: raw data is read and collected from various sources, including message queues, databases, flat files, spreadsheets, data streams, and event streams.

2. Transformation: business rules are applied to clean the data, enrich it, anonymize it, if necessary, and format it for analysis.

3. Loading: The transformed data is loaded into a big data store, such as a data warehouse, data lake, or non-relational database.

Traditional ETL has the following disadvantages:

1. Smaller extractionsdue to heavy processing of transformations.
2. Complexity: Traditional ETL is comprised of custom-coded programs and scripts, based on the specific needs of specific transformations. This means that the data engineering team must develop highly specialized, and often non-transferrable, skill sets for managing its code base.
3. Cost and time consumption: Once set up, adjusting the ETL process can be both costly and time consuming.
4. Rigidity: Traditional ETL limits the agility for data scientists.

## ELT

ELT stands for **extract-load-transform.** Unlike traditional ETL, ELT extracts and loads the data into the target first, where it runs transformations, often using proprietary scripting. The target is most commonly a data lake, or big data store, such as Teradata, Spark, or Hadoop.

The workflow:

1. Extraction: raw data is read and collected from various sources, including message queues, databases, flat files, spreadsheets, data streams, and event streams.

2. Loading: the extracted data is loaded into a data store, whether it is a data lake or warehouse, or non-relational database.

3. Transformation: data transformations are performed in the data lake or warehouse, primarily using scripts.

## Data lake and data warehouse

Data lakes and data warehouses are both widely used for storing big data, but they are not interchangeable terms. A data lake is a vast pool of raw data, the purpose for which is not yet defined. A data warehouse is a repository for structured, filtered data that has already been processed for a specific purpose.

## Data pipelines: steps

Data pipelines can also be explained as sequences of steps. Those steps often corrseponds with zones in a data lake.

### Genesis of the data

The genesis of the data is with the operational systems, such as streaming data collected from various Internet of Things devices or web session data from Google Analytics or some sales or CRM platforms, e.g., Salesforce. This data has to be stored somewhere, so that it can be processed at later times. Nowadays, the scale of the data and velocity at which it flows has lead to the rise of what we call “the data lake”.

### Operational data is stored in the landing zone

The data lake comprises several systems, and is typically organized in several zones. The data that comes from the operational systems for example, ends up in what we call the “landing zone”. This zone forms the basis of truth, it is always there and is the unaltered version of the data as it was received. The process of getting data into the data lake is called **ingestion**.

### Data cleansing and transformation - the clean zone

Data from the landing zone gets **cleansed** and stored in the clean zone. We’ll see in the next chapter what is typically meant by clean data.

### The business layer

Finally, some special transformations are applied to this clean data. For example, predicting which customers are likely to churn is a common business use case. You would apply a machine learning algorithm to a dataset composed of several cleaned datasets. This domain specific data is stored in the business layer.

## Diagrams Net starter for NYPD schema

Edit the diagram below and to complete a data pipeline diagram for a fictional organisation of your choice.

The data can be poured into either a data warehouse, e.g., BigQuery or a data lake.

Add text blocks to describe the organisation. It is a collaborative task.

All have permission to edit.

[<img
    src="/img/icons/diagrams_net_Logo.svg"
    alt="Link to a forkable diagram"
/>](https://drive.google.com/file/d/1FwidYjR0MDAx4CpMBGB4kEx2EpkIIa6K/view?usp=sharing)

[Link to diagrams net](https://drive.google.com/file/d/1FwidYjR0MDAx4CpMBGB4kEx2EpkIIa6K/view?usp=sharing)
